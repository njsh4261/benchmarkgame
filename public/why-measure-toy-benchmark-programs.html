<!DOCTYPE html>
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="robots" content="index,follow,archive">
<meta name="description" content="Why toy programs? You do have time to inspect 100-line programs. You do have time to write 100-line programs. ">
<title>Why toy programs? | Computer Language Benchmarks Game</title>
<link href="https://benchmarksgame-team.pages.debian.net/benchmarksgame/why-measure-toy-benchmark-programs.html" rel="canonical">
<link rel="shortcut icon" href="./darkfavicon.ico">
<style><!-- 
a{color:black;text-decoration:none}article,footer,header{margin:auto;max-width:31em;width:92%}body{font:100% Droid Sans,Ubuntu,Verdana,sans-serif;margin:0;-webkit-text-size-adjust:100%}h3,h1,h2{font-family:Ubuntu Mono,Consolas,Menlo,monospace}footer{padding:2.6em 0 0}h3{font-size:1.4em;font-weight:700;margin:0;padding:.4em}h3,h3 a{color:white;background-color:#2c001e}h3 small{font-size:.64em}h1,h2{margin:1.5em 0 0}h1{font-size:1.4em;font-weight:400}h2{font-size:1.2em}nav li{list-style-type:none;vertical-align:top}p{color:#333;line-height:1.4;margin:.3em 0 0}.external{border-bottom:.1em dashed #333}p a{border-bottom:.1em solid #333;padding-bottom:.1em}blockquote{margin-top:.3em}blockquote p{display:inline}blockquote:before,q:before{content:open-quote}blockquote:after,q:after{content:close-quote}body{quotes:'\201C''\201D''\2018''\2019''\2039''\203A'}@media only screen and (min-width:60em){article,footer,header{font-size:1.25em}}
--></style>
<header>
  <h3><a href="https://benchmarksgame-team.pages.debian.net/benchmarksgame/">The&nbsp;<small>Computer&nbsp;Language</small><br>Benchmarks&nbsp;Game</a></h3>
</header>
<article>
  <h1 id="toy-benchmark">Toy benchmark programs</h1>
  <h2><q>Computer Architecture: A Quantitative Approach</q></h2>
  <aside>
    <blockquote cite="https://books.google.com/books?id=v3-1hVwHnHwC&amp;printsec=frontcover&amp;source=gbs_ge_summary_r&amp;cad=0#v=onepage&amp;q&amp;f=false"><p>The best choice of benchmarks to measure performance is real applications&hellip; Attempts at running programs that are much simpler than a real application have led to performance pitfalls. Examples include:
      <ul>
        <li><p>Kernels, which are small, key pieces of real applications
        <li><p>Toy programs, which are 100-line programs from beginning programming assignments&hellip;
        <li><p>Synthetic benchmarks, which are small, fake programs invented to try to match the profile and behavior of real applications&hellip;
      </ul>
      <p>All three are <a class="external" href="https://books.google.com/books?id=v3-1hVwHnHwC&amp;lpg=PA37&amp;dq=hennessy%20patterson%20toy%20program&amp;pg=PA37#v=onepage&amp;q=%22The%20best%20choice%20of%20benchmarks%20to%20measure%20performance%22&amp;f=false">discredited today</a>, usually because the compiler writer and architect can conspire to make the computer appear faster on these stand-in programs than on real applications.</blockquote>
  </aside>
  <section>
    <h2 id="why-measure">So, why measure toy benchmark programs?</h2>
    <p>You don't have time to inspect the source code of real applications to check that different implementations are kind-of comparable. Is it <b>really</b> the same program when written in different programming languages?
    <p>You do have time to inspect 100-line programs. <a href="https://salsa.debian.org/benchmarksgame-team/benchmarksgame/blob/master/README.md">You do have time to write 100-line programs.</a> You still might have something to learn from how other people write 100-line programs.
    <p>Or even acknowledge — <q>Anyone else a bit shocked by how well Javascript on V8 performs? I might need to rethink my assumptions…</q>
    <h2 id="non-motivation">Non-motivation</h2>
    <p>We are profoundly uninterested in claims that these measurements, of a few tiny programs, somehow define the relative performance of programming languages <em>aka</em> <b><q>Which programming language is fastest?</q></b>.
  </section>
  <section>
    <h1 id="4chan">4chan</h1>
    <h2></h2>
    <blockquote cite="4chan"><p>My question is if anyone here has any experience with simplistic benchmarking and could tell me which things to test for in order to get a simple idea of each language's general performance?</blockquote>
  <aside>
    <h2>The practicalities depend on the context&hellip;</h2>
    <blockquote cite="https://www.cs.kent.ac.uk/pubs/2012/3233/"><p>The practicalities depend on the context (<a class="external" href="https://www.cs.kent.ac.uk/pubs/2012/3233/">the goal of the study</a>, the criticality of the results and the consequences of possible errors, the kind of computer system evaluated, the runtime environment, infrastructure, and benchmarks). We understand that having a single best technique that could be mechanically followed in every study might be appealing, but we strongly believe that such practice is an illusion, and we definitely do not provide one here.</blockquote>
  </aside>
  </section>
  <section>
    <h2 id="repetition">So explore!</h2>
    <p>Make <a class="external" href="http://www.itl.nist.gov/div898/handbook/eda/section3/runseqpl.htm">run-sequence charts</a> and check that the data is stable, without regularities or patterns.
    <p><img src="./download/run-sequence-nbody.svg" alt="" title="">
    <p>Make charts of the same data randomly re-ordered and check for systematic differences.
    <p><img src="./download/run-sequence-nbody-line.svg" alt="" title=""><img src="./download/run-sequence-nbody-reordered-line.svg" alt="" title="">
    <p>Make <a class="external" href="http://www.itl.nist.gov/div898/handbook/eda/section3/lagplot.htm">lag charts</a> and check. Make <a class="external" href="http://www.itl.nist.gov/div898/handbook/eda/section3/autocopl.htm">auto-correlation function</a> charts and check.
    <p><img src="./download/lag1-nbody.svg" alt="" title=""><img src="./download/lag1-nbody-reordered.svg" alt="" title="">
    <p><img src="./download/acf-nbody.svg" alt="" title=""><img src="./download/acf-nbody-reordered.svg" alt="" title="">
    <p>Explore the data to find out <a class="external" href="https://kar.kent.ac.uk/33611/">what kind-of repetition</a> and how much repetition is needed in the study.
    <p>In particular, don't assume that after an arbitrary <em>warmup</em> phase JIT compilers must improve program performance -- <em>It ain't necessarily so</em>.
    <blockquote cite="https://arxiv.org/abs/1602.00602"><p>In stark contrast to the traditional expectation of <q>warmup</q>, some benchmarks exhibit <a class="external" href="https://tratt.net/laurie/blog/entries/why_arent_more_users_more_happy_with_our_vms_part_1.html"><q>slowdown</q></a>, where the performance of in-process iterations drops over time.</blockquote> 
  </section>
  <section>
    <h2 id="cross-language-compiler-benchmarking">Initial comparisons</h2>
    <p>The 4chan poster needed to show comparisons between their academic research implementation and other programming language implementations. Like <a class="external" href="https://www.doc.ic.ac.uk/research/technicalreports/2018/DTRS18-4.pdf">[pdf] diverse</a> <a class="external" href="https://www.microsoft.com/en-us/research/uploads/prod/2019/09/static-typescript-camera-ready.pdf">researchers</a>, <a class="external" href="http://design.cs.iastate.edu/vmil/2013/papers/p05-Sarimbekov.pdf">over</a> <a class="external" href="https://sites.google.com/view/energy-efficiency-languages">the</a> <a class="external" href="http://janvitek.org/pubs/toplas18.pdf">past</a> <a class="external" href="https://riuma.uma.es/xmlui/bitstream/handle/10630/16910/idc.pdf">decade</a>, they may decide to use the benchmarks game as a <em>convenient</em> source of comparison programs.
    <p>Alternatively, for their initial assessment of compiler effectiveness, they may find the more-rigid less-permissive guidelines of <q><a class="external" href="https://github.com/smarr/are-we-fast-yet">Are We Fast Yet?</a></q> helpful.
    <p>If some of those guidelines seem arbitrary, a language implementor may feel that is less important than the possibility that there will be <em>identifiable</em> opportunities to improve performance.
  </section>
  <section>
    <h2 id="real-world"><q>The Real World</q></h2>
    <blockquote cite="https://www.microsoft.com/en-us/research/publication/jsmeter-characterizing-real-world-behavior-of-javascript-programs/"><p>&hellip; doing effective research on the Java platform requires benchmarks that are both real and more complex than benchmark suites used to evaluate languages like C, C++, and Fortran. <br/>&hellip;  the JavaScript benchmarks are <em>fleetingly small</em>, and behave in ways that are <em>significantly different</em> than the real applications. We have documented numerous differences in behavior, and we conclude from these measured differences that results based on the <a class="external" href="https://www.microsoft.com/en-us/research/publication/jsmeter-characterizing-real-world-behavior-of-javascript-programs/">benchmarks may mislead</a> JavaScript engine implementers. Furthermore, we observe interesting behaviors in real JavaScript applications that the benchmarks fail to exhibit, suggesting that previously unexplored optimization strategies may be productive in practice.</blockquote> 
    <p>The large number of projects available in public code repositories, along with the widespread adoption of unit testing in software development, have created <a class="external" href="https://www.researchgate.net/publication/299412274_AutoBench_Finding_Workloads_That_You_Need_Using_Pluggable_Hybrid_Analyses">a new source</a> of potential benchmark workloads.
    <p>For example, the <a class="external" href="https://arxiv.org/pdf/1903.10267.pdf">[pdf] Renaissance Benchmarking Suite</a> for Java — 
    <blockquote cite="https://arxiv.org/pdf/1903.10267.pdf"><p>To obtain these benchmarks, we gathered a large list of candidate workloads, both manually and by scanning an online corpus of GitHub projects.</blockquote>
  </section>
  <section>
    <h2 id="reimplement">Reimplement and measure your application</h2>
    <p>When different programming languages are to be evaluated as-implementation-tools for an existing application, in-house knowledge of current productivity and performance bottlenecks can be used to make the evaluation specific and relevant.
    <blockquote cite="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-2903-5"><p>These are full-fledged applications in the sense that they fully support a typical preparation pipeline for variant calling consisting of sorting reads, duplicate marking, and a few other commonly used steps. <a class="external" href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-2903-5">While these three reimplementations</a> &hellip; only support a limited set of functionality, in each case the software architecture could be completed with additional effort to support all features&hellip;</blockquote>
  </section>
  <section>
    <h2 id="citation">Citation</h2>
    <p>Gouy, Isaac. The Computer Language Benchmarks Game. Web.&lt;https://benchmarksgame-team.pages.debian.net/benchmarksgame/&gt;. 
  </section>
</article>
<footer>
  <nav>
    <ul>
      <li>&nbsp;
    </ul>
  </nav>
</footer>

